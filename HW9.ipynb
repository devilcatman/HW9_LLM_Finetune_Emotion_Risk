{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtYH4igfs-ue",
        "outputId": "2fcf900c-2e6f-4103-99d5-e6e5639a5e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.40.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.40.0) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"transformers>=4.40.0\" datasets accelerate peft bitsandbytes scikit-learn matplotlib pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rKdQjsX4C2M",
        "outputId": "07b005e5-26e6-44b1-c391-dadd5e79c61d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformers version: 4.57.1\n",
            "TrainingArguments module: transformers.training_args\n",
            "TrainingArguments.__init__ signature:\n",
            "(self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: float = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: bool = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: bool = True, label_names: Optional[list[str]] = None, load_best_model_at_end: bool = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = None, fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, parallelism_config: Optional[accelerate.parallelism_config.ParallelismConfig] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch_fused', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: str = 'length', report_to: Union[NoneType, str, list[str]] = None, project: str = 'huggingface', trackio_space_id: Optional[str] = 'trackio', ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: bool = False, include_num_input_tokens_seen: Union[str, bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: bool = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: bool = False, average_tokens_across_devices: bool = True) -> None\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import TrainingArguments\n",
        "import inspect\n",
        "\n",
        "print(\"transformers version:\", transformers.__version__)\n",
        "print(\"TrainingArguments module:\", TrainingArguments.__module__)\n",
        "print(\"TrainingArguments.__init__ signature:\")\n",
        "print(inspect.signature(TrainingArguments.__init__))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E9eXpvS0tCy8",
        "outputId": "e2977f46-7019-43ae-efd8-d19cefc3b9e9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p19eFk-BtVTV",
        "outputId": "30ce07e0-b1db-4018-b367-b58991dc9142"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 16000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"dair-ai/emotion\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlJ929qetd3_",
        "outputId": "89553fab-1d89-4dab-90cc-97ba7d6ee8c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_list = dataset[\"train\"].features[\"label\"].names\n",
        "label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iSBx7hZu796",
        "outputId": "0284a22d-932f-4e3a-e686-55b372f4cf3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'i didnt feel humiliated',\n",
              " 'label': 0,\n",
              " 'emotion': 'sadness',\n",
              " 'risk': 2}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 建立 emotion -> risk 的 mapping\n",
        "emotion_to_risk = {\n",
        "    \"joy\": 0,\n",
        "    \"love\": 0,\n",
        "    \"surprise\": 0,\n",
        "    \"anger\": 1,\n",
        "    \"fear\": 1,\n",
        "    \"sadness\": 2\n",
        "}\n",
        "\n",
        "def add_risk(example):\n",
        "    label_idx = example[\"label\"]\n",
        "    emotion = label_list[label_idx]\n",
        "    risk = emotion_to_risk[emotion]\n",
        "    example[\"emotion\"] = emotion\n",
        "    example[\"risk\"] = risk\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(add_risk)\n",
        "dataset[\"train\"][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-JLioqQvtGi",
        "outputId": "d302dcb3-ef88-46a6-bd24-5eabea106030"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-21): 22 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "base_model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abw_e161wBsE"
      },
      "outputs": [],
      "source": [
        "EMOTION_LABELS = [\"joy\", \"love\", \"surprise\", \"anger\", \"fear\", \"sadness\"]\n",
        "\n",
        "def build_zero_shot_prompt(text: str) -> str:\n",
        "    return f\"\"\"You are an assistant that classifies the emotion of a short social media post.\n",
        "\n",
        "Given the post, choose ONE emotion from: joy, love, surprise, anger, fear, sadness.\n",
        "\n",
        "Reply with only the emotion word.\n",
        "\n",
        "Post: {text}\n",
        "Emotion:\"\"\"\n",
        "\n",
        "def build_few_shot_prompt(text: str, examples) -> str:\n",
        "    \"\"\"\n",
        "    examples: list of dicts [{\"text\": ..., \"emotion\": ...}, ...]\n",
        "    \"\"\"\n",
        "    example_str = \"\"\n",
        "    for ex in examples:\n",
        "        example_str += f\"\"\"Post: {ex[\"text\"]}\n",
        "Emotion: {ex[\"emotion\"]}\n",
        "\n",
        "\"\"\"\n",
        "    return f\"\"\"You are an assistant that classifies the emotion of a short social media post.\n",
        "\n",
        "Choose ONE emotion from: joy, love, surprise, anger, fear, sadness.\n",
        "\n",
        "Here are some examples:\n",
        "{example_str}\n",
        "Now classify the following post. Reply with only the emotion word.\n",
        "\n",
        "Post: {text}\n",
        "Emotion:\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAPgKQT8wPBN"
      },
      "outputs": [],
      "source": [
        "def decode_emotion_from_output(output_text: str) -> str:\n",
        "    # 簡單找出第一個出現的 emotion 字\n",
        "    output_text = output_text.lower()\n",
        "    for emo in EMOTION_LABELS:\n",
        "        if emo in output_text:\n",
        "            return emo\n",
        "    # fallback: 如果完全沒找到，就標最常見的 low risk 情緒 'joy'\n",
        "    return \"joy\"\n",
        "\n",
        "def emotion_to_risk_label(emotion: str) -> int:\n",
        "    return emotion_to_risk[emotion]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epIonA4YwSBu"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate_emotion(prompts, model, tokenizer, max_new_tokens=8):\n",
        "    \"\"\"\n",
        "    prompts: list[str]\n",
        "    回傳: list[emotion_str]\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False\n",
        "    )\n",
        "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # 從完整輸出中切出最後一行當作回答\n",
        "    emotions = []\n",
        "    for full in decoded:\n",
        "        # 取最後一個 \"Emotion:\" 後面的內容\n",
        "        if \"Emotion:\" in full:\n",
        "            ans = full.split(\"Emotion:\")[-1].strip()\n",
        "        else:\n",
        "            ans = full.strip()\n",
        "        emo = decode_emotion_from_output(ans)\n",
        "        emotions.append(emo)\n",
        "    return emotions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "36aaab73eb88443b98497746937e6534",
            "bb138c17abca41dd8c488f7b7d155737",
            "480fe8a7b0674799a80d5e789e2ff767",
            "0f146ad5701a4b17abb91f5409255a8e",
            "f78ee0753f1a4cffbae42a71eef61e2f",
            "dfbb4cc5c25e4c4aba1a58dcf615c31a",
            "0cee0ab6df194a1bb71e3ece2c3cd610",
            "8b6cf724f2ba47b39b3ab0f6d2eb5f28",
            "9cecd09712a84a1bb67e87cf1d32b089",
            "46597c098b574ca2bf271d41efbe887a",
            "61b955cbd0a14ae1a7b74df9334abd2e"
          ]
        },
        "id": "LvOe7HC4wXWN",
        "outputId": "99ed1121-935a-4391-fb88-b1a6b3221539"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36aaab73eb88443b98497746937e6534",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(500, ['joy', 'joy', 'joy', 'joy', 'joy'])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from math import ceil\n",
        "\n",
        "test_data = dataset[\"test\"]\n",
        "\n",
        "# 為了 demo，先取前 N 筆\n",
        "N = 500   # 可以改 2000\n",
        "texts = test_data[\"text\"][:N]\n",
        "true_emotion = test_data[\"emotion\"][:N]\n",
        "true_risk = test_data[\"risk\"][:N]\n",
        "\n",
        "zero_pred_emotion = []\n",
        "batch_size = 8\n",
        "\n",
        "for i in tqdm(range(0, N, batch_size)):\n",
        "    batch_texts = texts[i:i+batch_size]\n",
        "    prompts = [build_zero_shot_prompt(t) for t in batch_texts]\n",
        "    batch_preds = generate_emotion(prompts, base_model, tokenizer)\n",
        "    zero_pred_emotion.extend(batch_preds)\n",
        "\n",
        "len(zero_pred_emotion), zero_pred_emotion[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvB_GEfhwZ97"
      },
      "outputs": [],
      "source": [
        "# emotion → 整數 label / risk label\n",
        "emo_to_idx = {e: i for i, e in enumerate(EMOTION_LABELS)}\n",
        "\n",
        "y_true_emo = [emo_to_idx[e] for e in true_emotion]\n",
        "y_pred_emo_zero = [emo_to_idx[e] for e in zero_pred_emotion]\n",
        "\n",
        "y_true_risk = true_risk\n",
        "y_pred_risk_zero = [emotion_to_risk_label(e) for e in zero_pred_emotion]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIWzqb2swcMT",
        "outputId": "f2952828-a5ca-4e30-c591-19c5bc28d619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Zero-shot Emotion Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3174    0.9669    0.4779       151\n",
            "           1     0.0000    0.0000    0.0000        39\n",
            "           2     0.0000    0.0000    0.0000        13\n",
            "           3     0.4000    0.0506    0.0899        79\n",
            "           4     1.0000    0.0429    0.0822        70\n",
            "           5     0.4444    0.0541    0.0964       148\n",
            "\n",
            "    accuracy                         0.3220       500\n",
            "   macro avg     0.3603    0.1857    0.1244       500\n",
            "weighted avg     0.4306    0.3220    0.1986       500\n",
            "\n",
            "F1-macro  : 0.1244\n",
            "F1-weight : 0.1986\n",
            "AUROC     : 0.5128\n",
            "PR-AUC    : 0.1787\n",
            "Confusion Matrix:\n",
            "[[146   1   1   1   0   2]\n",
            " [ 35   0   0   2   0   2]\n",
            " [ 13   0   0   0   0   0]\n",
            " [ 67   3   1   4   0   4]\n",
            " [ 61   0   1   3   3   2]\n",
            " [138   0   2   0   0   8]]\n",
            "=== Zero-shot Risk Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4179    0.9655    0.5833       203\n",
            "           1     0.7692    0.0671    0.1235       149\n",
            "           2     0.4444    0.0541    0.0964       148\n",
            "\n",
            "    accuracy                         0.4280       500\n",
            "   macro avg     0.5439    0.3622    0.2677       500\n",
            "weighted avg     0.5305    0.4280    0.3022       500\n",
            "\n",
            "F1-macro  : 0.2677\n",
            "F1-weight : 0.3022\n",
            "AUROC     : 0.5218\n",
            "PR-AUC    : 0.3504\n",
            "Confusion Matrix:\n",
            "[[196   3   4]\n",
            " [133  10   6]\n",
            " [140   0   8]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def evaluate_classification(y_true, y_pred, num_classes, average=\"macro\", task_name=\"\"):\n",
        "    print(f\"=== {task_name} Classification Report ===\")\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "    # F1\n",
        "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "    # one-vs-rest ROC / PR\n",
        "    y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))\n",
        "    y_pred_bin = label_binarize(y_pred, classes=list(range(num_classes)))\n",
        "\n",
        "    try:\n",
        "        roc = roc_auc_score(y_true_bin, y_pred_bin, average=\"macro\", multi_class=\"ovr\")\n",
        "    except ValueError:\n",
        "        roc = np.nan\n",
        "\n",
        "    try:\n",
        "        pr_auc = average_precision_score(y_true_bin, y_pred_bin, average=\"macro\")\n",
        "    except ValueError:\n",
        "        pr_auc = np.nan\n",
        "\n",
        "    print(f\"F1-macro  : {f1_macro:.4f}\")\n",
        "    print(f\"F1-weight : {f1_weighted:.4f}\")\n",
        "    print(f\"AUROC     : {roc:.4f}\")\n",
        "    print(f\"PR-AUC    : {pr_auc:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    return {\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"f1_weighted\": f1_weighted,\n",
        "        \"auroc\": roc,\n",
        "        \"pr_auc\": pr_auc,\n",
        "        \"confusion_matrix\": cm\n",
        "    }\n",
        "\n",
        "metrics_zero_emo = evaluate_classification(\n",
        "    y_true_emo, y_pred_emo_zero, num_classes=len(EMOTION_LABELS), task_name=\"Zero-shot Emotion\"\n",
        ")\n",
        "\n",
        "metrics_zero_risk = evaluate_classification(\n",
        "    y_true_risk, y_pred_risk_zero, num_classes=3, task_name=\"Zero-shot Risk\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl4g8zgUweml",
        "outputId": "b632fce8-b76e-47a2-bef3-fc79475d620f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'text': 'i didnt feel humiliated', 'emotion': 'sadness'},\n",
              " {'text': 'im grabbing a minute to post i feel greedy wrong',\n",
              "  'emotion': 'anger'},\n",
              " {'text': 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n",
              "  'emotion': 'love'},\n",
              " {'text': 'ive been taking or milligrams or times recommended amount and ive fallen asleep a lot faster but i also feel like so funny',\n",
              "  'emotion': 'surprise'},\n",
              " {'text': 'i feel as confused about life as a teenager or as jaded as a year old man',\n",
              "  'emotion': 'fear'},\n",
              " {'text': 'i have been with petronas for years i feel that petronas has performed well and made a huge profit',\n",
              "  'emotion': 'joy'}]"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 從 train 抽 few-shot 示例，每個 emotion 抽 1 個\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "few_shot_examples = []\n",
        "selected_emotions = set()\n",
        "\n",
        "for ex in train_data:\n",
        "    emo = ex[\"emotion\"]\n",
        "    if emo not in selected_emotions:\n",
        "        few_shot_examples.append({\"text\": ex[\"text\"], \"emotion\": emo})\n",
        "        selected_emotions.add(emo)\n",
        "    if len(selected_emotions) == len(EMOTION_LABELS):\n",
        "        break\n",
        "\n",
        "few_shot_examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "bf8402c184064dfea945da736649290d",
            "41769b8c6f0540698813347447340ce6",
            "4232bb97f1bc40029c313b71cb4855f2",
            "3148a4620b6145bd89bbbd63ed56131c",
            "f30f1d2af5ff404186c1aecf9ca660ef",
            "3295b9412e7c4e4cb92312acb768ea7b",
            "3c586dd51ce74b8fbdad6428b1517a9f",
            "50c98d845d2441cd96b9b1ab70479f3c",
            "19d16a733df14783818b00ed9a06a24e",
            "32521f1b69a040259bbf381a922375c4",
            "fd08d353b9ec4f789c4e96ab9c0b536c"
          ]
        },
        "id": "C9_OOukTwgzG",
        "outputId": "ed7f2feb-8a19-4f07-dbf0-424e44fc1840"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf8402c184064dfea945da736649290d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(500, ['joy', 'joy', 'joy', 'joy', 'joy'])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "few_pred_emotion = []\n",
        "\n",
        "for i in tqdm(range(0, N, batch_size)):\n",
        "    batch_texts = texts[i:i+batch_size]\n",
        "    prompts = [build_few_shot_prompt(t, few_shot_examples) for t in batch_texts]\n",
        "    batch_preds = generate_emotion(prompts, base_model, tokenizer)\n",
        "    few_pred_emotion.extend(batch_preds)\n",
        "\n",
        "len(few_pred_emotion), few_pred_emotion[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC5ULfqvwi6l",
        "outputId": "9e747d83-d167-4c7c-ce80-1f4e8abcb30c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Few-shot Emotion Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3103    0.8940    0.4608       151\n",
            "           1     0.0000    0.0000    0.0000        39\n",
            "           2     0.0417    0.0769    0.0541        13\n",
            "           3     0.5238    0.1392    0.2200        79\n",
            "           4     1.0000    0.0857    0.1579        70\n",
            "           5     0.7000    0.0473    0.0886       148\n",
            "\n",
            "    accuracy                         0.3200       500\n",
            "   macro avg     0.4293    0.2072    0.1636       500\n",
            "weighted avg     0.5248    0.3200    0.2236       500\n",
            "\n",
            "F1-macro  : 0.1636\n",
            "F1-weight : 0.2236\n",
            "AUROC     : 0.5246\n",
            "PR-AUC    : 0.1921\n",
            "Confusion Matrix:\n",
            "[[135   3  12   1   0   0]\n",
            " [ 35   0   0   4   0   0]\n",
            " [ 12   0   1   0   0   0]\n",
            " [ 63   0   4  11   0   1]\n",
            " [ 57   0   3   2   6   2]\n",
            " [133   1   4   3   0   7]]\n",
            "=== Few-shot Risk Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4276    0.9754    0.5946       203\n",
            "           1     0.7037    0.1275    0.2159       149\n",
            "           2     0.7000    0.0473    0.0886       148\n",
            "\n",
            "    accuracy                         0.4480       500\n",
            "   macro avg     0.6104    0.3834    0.2997       500\n",
            "weighted avg     0.5905    0.4480    0.3320       500\n",
            "\n",
            "F1-macro  : 0.2997\n",
            "F1-weight : 0.3320\n",
            "AUROC     : 0.5378\n",
            "PR-AUC    : 0.3640\n",
            "Confusion Matrix:\n",
            "[[198   5   0]\n",
            " [127  19   3]\n",
            " [138   3   7]]\n"
          ]
        }
      ],
      "source": [
        "y_pred_emo_few = [emo_to_idx[e] for e in few_pred_emotion]\n",
        "y_pred_risk_few = [emotion_to_risk_label(e) for e in few_pred_emotion]\n",
        "\n",
        "metrics_few_emo = evaluate_classification(\n",
        "    y_true_emo, y_pred_emo_few, num_classes=len(EMOTION_LABELS), task_name=\"Few-shot Emotion\"\n",
        ")\n",
        "\n",
        "metrics_few_risk = evaluate_classification(\n",
        "    y_true_risk, y_pred_risk_few, num_classes=3, task_name=\"Few-shot Risk\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VSyjCKhwlbl",
        "outputId": "2e321d88-b7a4-4706-d8f7-349572cefe97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16000,\n",
              " 'You are an assistant that classifies the emotion of a short social media post.\\n\\nPost: i didnt feel humiliated\\nEmotion:',\n",
              " 'sadness')"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def build_train_example(text, emotion):\n",
        "    # 簡單的指令格式\n",
        "    return f\"\"\"You are an assistant that classifies the emotion of a short social media post.\n",
        "\n",
        "Post: {text}\n",
        "Emotion:\"\"\", emotion\n",
        "\n",
        "def convert_to_sft_format(split):\n",
        "    prompts = []\n",
        "    targets = []\n",
        "    for ex in split:\n",
        "        p, t = build_train_example(ex[\"text\"], ex[\"emotion\"])\n",
        "        prompts.append(p)\n",
        "        targets.append(t)\n",
        "    return prompts, targets\n",
        "\n",
        "train_prompts, train_targets = convert_to_sft_format(dataset[\"train\"])\n",
        "val_prompts, val_targets = convert_to_sft_format(dataset[\"validation\"])\n",
        "\n",
        "len(train_prompts), train_prompts[0], train_targets[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0rHd1-QwnQU",
        "outputId": "b479e92b-df54-452a-fd63-11bddf47cfae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # 針對 attention layer\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "lora_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "lora_model = prepare_model_for_kbit_training(lora_model)\n",
        "lora_model = get_peft_model(lora_model, lora_config)\n",
        "lora_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET36lzQvwqSj",
        "outputId": "a25dc699-3ca4-4853-beaa-bf98677ed5cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16000"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class EmotionSFTDataset(Dataset):\n",
        "    def __init__(self, prompts, targets, tokenizer, max_length=256):\n",
        "        self.prompts = prompts\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prompts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt = self.prompts[idx]\n",
        "        target = self.targets[idx]\n",
        "        # 我們希望模型學「prompt + target」，target 部分當 label\n",
        "        full_text = prompt + \" \" + target\n",
        "        tokenized = self.tokenizer(\n",
        "            full_text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = tokenized[\"input_ids\"][0]\n",
        "        attention_mask = tokenized[\"attention_mask\"][0]\n",
        "        # label = input_ids（一般 SFT 作法）\n",
        "        labels = input_ids.clone()\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "train_sft_ds = EmotionSFTDataset(train_prompts, train_targets, tokenizer)\n",
        "val_sft_ds = EmotionSFTDataset(val_prompts, val_targets, tokenizer)\n",
        "len(train_sft_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7l_W_r53itb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vxLmTqVbwtbr",
        "outputId": "fb61260f-bcdd-4be9-ec59-f89eed628715"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 42:55, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.809000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.307000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.287300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.292900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.289900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.281000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.289900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.288500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.286200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.281300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.281100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.276200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.279200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.280300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.288700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.275800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.280500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.275900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.286000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.287300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=0.41119957065582274, metrics={'train_runtime': 2578.7289, 'train_samples_per_second': 6.205, 'train_steps_per_second': 0.388, 'total_flos': 2.5479541161984e+16, 'train_loss': 0.41119957065582274, 'epoch': 1.0})"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "output_dir = \"./tinyllama-emotion-lora\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=50,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",              # 關掉 wandb / tensorboard 等 logger\n",
        "    fp16=torch.cuda.is_available() # 或 bf16 也可以，但 fp16 比較穩\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_sft_ds,\n",
        "    eval_dataset=val_sft_ds,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sjgmt-Nqwv-c"
      },
      "outputs": [],
      "source": [
        "lora_model.eval()\n",
        "\n",
        "lora_pred_emotion = []\n",
        "\n",
        "for i in tqdm(range(0, N, batch_size)):\n",
        "    batch_texts = texts[i:i+batch_size]\n",
        "    prompts = [build_zero_shot_prompt(t) for t in batch_texts]  # 同樣的 prompt，但 model 換成 lora_model\n",
        "    batch_preds = generate_emotion(prompts, lora_model, tokenizer)\n",
        "    lora_pred_emotion.extend(batch_preds)\n",
        "\n",
        "len(lora_pred_emotion), lora_pred_emotion[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUfRD35QxA4A"
      },
      "outputs": [],
      "source": [
        "y_pred_emo_lora = [emo_to_idx[e] for e in lora_pred_emotion]\n",
        "y_pred_risk_lora = [emotion_to_risk_label(e) for e in lora_pred_emotion]\n",
        "\n",
        "metrics_lora_emo = evaluate_classification(\n",
        "    y_true_emo, y_pred_emo_lora, num_classes=len(EMOTION_LABELS), task_name=\"LoRA Emotion\"\n",
        ")\n",
        "\n",
        "metrics_lora_risk = evaluate_classification(\n",
        "    y_true_risk, y_pred_risk_lora, num_classes=3, task_name=\"LoRA Risk\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qK3MvlyxCyF"
      },
      "outputs": [],
      "source": [
        "# 簡單把 risk label 映射成 \"高風險機率\"\n",
        "def risk_to_phigh(risk_label: int) -> float:\n",
        "    if risk_label == 2:\n",
        "        return 1.0\n",
        "    elif risk_label == 1:\n",
        "        return 0.5\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "p_high_lora = np.array([risk_to_phigh(r) for r in y_pred_risk_lora])\n",
        "\n",
        "# 建一個 DataFrame 方便畫圖\n",
        "df_risk = pd.DataFrame({\n",
        "    \"index\": np.arange(N),\n",
        "    \"risk_true\": y_true_risk,\n",
        "    \"risk_pred_lora\": y_pred_risk_lora,\n",
        "    \"p_high_lora\": p_high_lora\n",
        "})\n",
        "\n",
        "df_risk.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwJXbiUHxGFY"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(df_risk[\"index\"], df_risk[\"p_high_lora\"], marker=\"o\", linestyle=\"-\", linewidth=1)\n",
        "plt.xlabel(\"Sample index\")\n",
        "plt.ylabel(\"P(high_risk)\")\n",
        "plt.title(\"High Risk Probability over Samples (LoRA)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS3LyXqbxIXI"
      },
      "outputs": [],
      "source": [
        "window_size = 50\n",
        "df_risk[\"p_high_roll\"] = df_risk[\"p_high_lora\"].rolling(window=window_size, min_periods=1).mean()\n",
        "\n",
        "# 把 rolling 結果 reshape 成 2D，畫 heatmap\n",
        "# 例如每 row 50 個樣本\n",
        "step = window_size\n",
        "num_rows = int(np.ceil(N / step))\n",
        "heat_data = []\n",
        "\n",
        "for i in range(num_rows):\n",
        "    segment = df_risk[\"p_high_roll\"].iloc[i*step:(i+1)*step].to_numpy()\n",
        "    # 補齊長度\n",
        "    if len(segment) < step:\n",
        "        segment = np.pad(segment, (0, step - len(segment)), constant_values=np.nan)\n",
        "    heat_data.append(segment)\n",
        "\n",
        "heat_data = np.array(heat_data)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(heat_data, aspect=\"auto\", interpolation=\"nearest\")\n",
        "plt.colorbar(label=\"P(high_risk) (rolling mean)\")\n",
        "plt.xlabel(\"Index within window\")\n",
        "plt.ylabel(\"Window #\")\n",
        "plt.title(f\"High Risk Heatmap (window={window_size})\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nJVrOQ-xKLK"
      },
      "outputs": [],
      "source": [
        "summary_emo = pd.DataFrame([\n",
        "    {\"method\": \"zero-shot\", \"task\": \"emotion\", **metrics_zero_emo},\n",
        "    {\"method\": \"few-shot\", \"task\": \"emotion\", **metrics_few_emo},\n",
        "    {\"method\": \"LoRA\", \"task\": \"emotion\", **metrics_lora_emo},\n",
        "])\n",
        "\n",
        "summary_risk = pd.DataFrame([\n",
        "    {\"method\": \"zero-shot\", \"task\": \"risk\", **metrics_zero_risk},\n",
        "    {\"method\": \"few-shot\", \"task\": \"risk\", **metrics_few_risk},\n",
        "    {\"method\": \"LoRA\", \"task\": \"risk\", **metrics_lora_risk},\n",
        "])\n",
        "\n",
        "summary_emo, summary_risk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK18EwbStBPT"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cee0ab6df194a1bb71e3ece2c3cd610": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f146ad5701a4b17abb91f5409255a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46597c098b574ca2bf271d41efbe887a",
            "placeholder": "​",
            "style": "IPY_MODEL_61b955cbd0a14ae1a7b74df9334abd2e",
            "value": " 63/63 [02:12&lt;00:00,  1.55s/it]"
          }
        },
        "19d16a733df14783818b00ed9a06a24e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3148a4620b6145bd89bbbd63ed56131c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32521f1b69a040259bbf381a922375c4",
            "placeholder": "​",
            "style": "IPY_MODEL_fd08d353b9ec4f789c4e96ab9c0b536c",
            "value": " 63/63 [02:56&lt;00:00,  2.46s/it]"
          }
        },
        "32521f1b69a040259bbf381a922375c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3295b9412e7c4e4cb92312acb768ea7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36aaab73eb88443b98497746937e6534": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb138c17abca41dd8c488f7b7d155737",
              "IPY_MODEL_480fe8a7b0674799a80d5e789e2ff767",
              "IPY_MODEL_0f146ad5701a4b17abb91f5409255a8e"
            ],
            "layout": "IPY_MODEL_f78ee0753f1a4cffbae42a71eef61e2f"
          }
        },
        "3c586dd51ce74b8fbdad6428b1517a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41769b8c6f0540698813347447340ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3295b9412e7c4e4cb92312acb768ea7b",
            "placeholder": "​",
            "style": "IPY_MODEL_3c586dd51ce74b8fbdad6428b1517a9f",
            "value": "100%"
          }
        },
        "4232bb97f1bc40029c313b71cb4855f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50c98d845d2441cd96b9b1ab70479f3c",
            "max": 63,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19d16a733df14783818b00ed9a06a24e",
            "value": 63
          }
        },
        "46597c098b574ca2bf271d41efbe887a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "480fe8a7b0674799a80d5e789e2ff767": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b6cf724f2ba47b39b3ab0f6d2eb5f28",
            "max": 63,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cecd09712a84a1bb67e87cf1d32b089",
            "value": 63
          }
        },
        "50c98d845d2441cd96b9b1ab70479f3c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61b955cbd0a14ae1a7b74df9334abd2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b6cf724f2ba47b39b3ab0f6d2eb5f28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cecd09712a84a1bb67e87cf1d32b089": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb138c17abca41dd8c488f7b7d155737": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfbb4cc5c25e4c4aba1a58dcf615c31a",
            "placeholder": "​",
            "style": "IPY_MODEL_0cee0ab6df194a1bb71e3ece2c3cd610",
            "value": "100%"
          }
        },
        "bf8402c184064dfea945da736649290d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41769b8c6f0540698813347447340ce6",
              "IPY_MODEL_4232bb97f1bc40029c313b71cb4855f2",
              "IPY_MODEL_3148a4620b6145bd89bbbd63ed56131c"
            ],
            "layout": "IPY_MODEL_f30f1d2af5ff404186c1aecf9ca660ef"
          }
        },
        "dfbb4cc5c25e4c4aba1a58dcf615c31a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f30f1d2af5ff404186c1aecf9ca660ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f78ee0753f1a4cffbae42a71eef61e2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd08d353b9ec4f789c4e96ab9c0b536c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}